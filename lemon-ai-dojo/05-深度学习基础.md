# 深度学习基础

## 💡 核心结论

1. **神经网络通过多层非线性变换学习复杂特征**
2. **反向传播算法通过链式法则计算梯度**
3. **激活函数引入非线性，ReLU是最常用的**
4. **Dropout和BatchNorm是防止过拟合的利器**
5. **GPU加速使深度学习训练成为可能**

---

## 1. 神经网络基础

### 1.1 神经元

```
输入：x₁, x₂, ..., xₙ
权重：w₁, w₂, ..., wₙ
偏置：b

z = Σ(wᵢxᵢ) + b
输出：a = σ(z)  # σ是激活函数
```

### 1.2 前向传播

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def forward(X, W1, b1, W2, b2):
    # 第一层
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    
    # 第二层
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    
    return A2
```

### 1.3 反向传播

```python
def backward(X, y, A1, A2, W2):
    m = X.shape[0]
    
    # 输出层梯度
    dZ2 = A2 - y
    dW2 = (1/m) * np.dot(A1.T, dZ2)
    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)
    
    # 隐藏层梯度
    dZ1 = np.dot(dZ2, W2.T) * A1 * (1 - A1)
    dW1 = (1/m) * np.dot(X.T, dZ1)
    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)
    
    return dW1, db1, dW2, db2
```

---

## 2. 激活函数

### 2.1 常用激活函数

```python
# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh
def tanh(x):
    return np.tanh(x)

# ReLU（最常用）
def relu(x):
    return np.maximum(0, x)

# Leaky ReLU
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# Softmax（多分类输出层）
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

---

## 3. PyTorch实现

### 3.1 简单神经网络

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义模型
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 创建模型
model = SimpleNN(input_size=10, hidden_size=64, output_size=2)

# 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练
for epoch in range(100):
    # 前向传播
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')

# 预测
with torch.no_grad():
    predictions = model(X_test)
    _, predicted = torch.max(predictions, 1)
```

---

## 4. 正则化技术

### 4.1 Dropout

```python
class NNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x
```

### 4.2 Batch Normalization

```python
class NNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.fc3(x)
        return x
```

---

## 参考资源

- PyTorch官方教程
- 《深度学习》- Goodfellow

