# æ·±åº¦å­¦ä¹ åŸºç¡€

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **ç¥ç»ç½‘ç»œé€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢å­¦ä¹ å¤æ‚ç‰¹å¾**
2. **åå‘ä¼ æ’­ç®—æ³•é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦**
3. **æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼ŒReLUæ˜¯æœ€å¸¸ç”¨çš„**
4. **Dropoutå’ŒBatchNormæ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„åˆ©å™¨**
5. **GPUåŠ é€Ÿä½¿æ·±åº¦å­¦ä¹ è®­ç»ƒæˆä¸ºå¯èƒ½**

---

## 1. ç¥ç»ç½‘ç»œåŸºç¡€

### 1.1 ç¥ç»å…ƒ

```
è¾“å…¥ï¼šxâ‚, xâ‚‚, ..., xâ‚™
æƒé‡ï¼šwâ‚, wâ‚‚, ..., wâ‚™
åç½®ï¼šb

z = Î£(wáµ¢xáµ¢) + b
è¾“å‡ºï¼ša = Ïƒ(z)  # Ïƒæ˜¯æ¿€æ´»å‡½æ•°
```

### 1.2 å‰å‘ä¼ æ’­

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def forward(X, W1, b1, W2, b2):
    # ç¬¬ä¸€å±‚
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    
    # ç¬¬äºŒå±‚
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    
    return A2
```

### 1.3 åå‘ä¼ æ’­

```python
def backward(X, y, A1, A2, W2):
    m = X.shape[0]
    
    # è¾“å‡ºå±‚æ¢¯åº¦
    dZ2 = A2 - y
    dW2 = (1/m) * np.dot(A1.T, dZ2)
    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)
    
    # éšè—å±‚æ¢¯åº¦
    dZ1 = np.dot(dZ2, W2.T) * A1 * (1 - A1)
    dW1 = (1/m) * np.dot(X.T, dZ1)
    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)
    
    return dW1, db1, dW2, db2
```

---

## 2. æ¿€æ´»å‡½æ•°

### 2.1 å¸¸ç”¨æ¿€æ´»å‡½æ•°

```python
# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh
def tanh(x):
    return np.tanh(x)

# ReLUï¼ˆæœ€å¸¸ç”¨ï¼‰
def relu(x):
    return np.maximum(0, x)

# Leaky ReLU
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# Softmaxï¼ˆå¤šåˆ†ç±»è¾“å‡ºå±‚ï¼‰
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

---

## 3. PyTorchå®ç°

### 3.1 ç®€å•ç¥ç»ç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰æ¨¡å‹
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = SimpleNN(input_size=10, hidden_size=64, output_size=2)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
for epoch in range(100):
    # å‰å‘ä¼ æ’­
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')

# é¢„æµ‹
with torch.no_grad():
    predictions = model(X_test)
    _, predicted = torch.max(predictions, 1)
```

---

## 4. æ­£åˆ™åŒ–æŠ€æœ¯

### 4.1 Dropout

```python
class NNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x
```

### 4.2 Batch Normalization

```python
class NNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.fc3(x)
        return x
```

---

## å‚è€ƒèµ„æº

- PyTorchå®˜æ–¹æ•™ç¨‹
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Goodfellow

