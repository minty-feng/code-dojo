# AI模型部署

## 💡 核心结论

1. **模型需要转换为ONNX等格式才能跨平台部署**
2. **模型量化可以减小体积和提升推理速度**
3. **TensorRT、OpenVINO等推理引擎优化性能**
4. **Flask/FastAPI快速构建模型API服务**
5. **Docker容器化部署，Kubernetes编排管理**

---

## 1. 模型导出

### 1.1 PyTorch转ONNX

```python
import torch

# 加载模型
model = torch.load('model.pth')
model.eval()

# 导出ONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)
```

### 1.2 使用ONNX Runtime

```python
import onnxruntime as ort
import numpy as np

# 加载模型
session = ort.InferenceSession("model.onnx")

# 推理
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
output = session.run([output_name], {input_name: input_data})

print(output[0])
```

---

## 2. 模型API服务

### 2.1 Flask部署

```python
from flask import Flask, request, jsonify
import torch
from PIL import Image
import io

app = Flask(__name__)

# 加载模型
model = torch.load('model.pth')
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    image = Image.open(io.BytesIO(file.read()))
    
    # 预处理
    input_tensor = transform(image).unsqueeze(0)
    
    # 推理
    with torch.no_grad():
        output = model(input_tensor)
        prediction = output.argmax(1).item()
    
    return jsonify({'prediction': prediction})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 2.2 FastAPI部署

```python
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
import torch
from PIL import Image
import io

app = FastAPI()

model = torch.load('model.pth')
model.eval()

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
    
    input_tensor = transform(image).unsqueeze(0)
    
    with torch.no_grad():
        output = model(input_tensor)
        prediction = output.argmax(1).item()
    
    return JSONResponse({"prediction": prediction})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 3. Docker部署

### 3.1 Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码和模型
COPY app.py .
COPY model.pth .
COPY utils/ ./utils/

EXPOSE 5000

CMD ["python", "app.py"]
```

### 3.2 构建和运行

```bash
# 构建镜像
docker build -t ml-api:v1 .

# 运行容器
docker run -d -p 5000:5000 --name ml-service ml-api:v1

# 测试
curl -X POST -F "file=@test.jpg" http://localhost:5000/predict
```

---

## 4. 模型优化

### 4.1 量化

```python
import torch

# 动态量化
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

torch.save(model_quantized.state_dict(), 'model_quantized.pth')

# 模型大小对比
import os
print(f"原始模型: {os.path.getsize('model.pth') / 1024 / 1024:.2f} MB")
print(f"量化模型: {os.path.getsize('model_quantized.pth') / 1024 / 1024:.2f} MB")
```

---

## 参考资源

- ONNX官方文档
- FastAPI文档
- Docker官方文档

