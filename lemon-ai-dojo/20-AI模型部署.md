# AIæ¨¡å‹éƒ¨ç½²

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **æ¨¡å‹éœ€è¦è½¬æ¢ä¸ºONNXç­‰æ ¼å¼æ‰èƒ½è·¨å¹³å°éƒ¨ç½²**
2. **æ¨¡å‹é‡åŒ–å¯ä»¥å‡å°ä½“ç§¯å’Œæå‡æ¨ç†é€Ÿåº¦**
3. **TensorRTã€OpenVINOç­‰æ¨ç†å¼•æ“ä¼˜åŒ–æ€§èƒ½**
4. **Flask/FastAPIå¿«é€Ÿæ„å»ºæ¨¡å‹APIæœåŠ¡**
5. **Dockerå®¹å™¨åŒ–éƒ¨ç½²ï¼ŒKubernetesç¼–æ’ç®¡ç†**

---

## 1. æ¨¡å‹å¯¼å‡º

### 1.1 PyTorchè½¬ONNX

```python
import torch

# åŠ è½½æ¨¡å‹
model = torch.load('model.pth')
model.eval()

# å¯¼å‡ºONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)
```

### 1.2 ä½¿ç”¨ONNX Runtime

```python
import onnxruntime as ort
import numpy as np

# åŠ è½½æ¨¡å‹
session = ort.InferenceSession("model.onnx")

# æ¨ç†
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
output = session.run([output_name], {input_name: input_data})

print(output[0])
```

---

## 2. æ¨¡å‹APIæœåŠ¡

### 2.1 Flaskéƒ¨ç½²

```python
from flask import Flask, request, jsonify
import torch
from PIL import Image
import io

app = Flask(__name__)

# åŠ è½½æ¨¡å‹
model = torch.load('model.pth')
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    image = Image.open(io.BytesIO(file.read()))
    
    # é¢„å¤„ç†
    input_tensor = transform(image).unsqueeze(0)
    
    # æ¨ç†
    with torch.no_grad():
        output = model(input_tensor)
        prediction = output.argmax(1).item()
    
    return jsonify({'prediction': prediction})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 2.2 FastAPIéƒ¨ç½²

```python
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
import torch
from PIL import Image
import io

app = FastAPI()

model = torch.load('model.pth')
model.eval()

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
    
    input_tensor = transform(image).unsqueeze(0)
    
    with torch.no_grad():
        output = model(input_tensor)
        prediction = output.argmax(1).item()
    
    return JSONResponse({"prediction": prediction})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 3. Dockeréƒ¨ç½²

### 3.1 Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç å’Œæ¨¡å‹
COPY app.py .
COPY model.pth .
COPY utils/ ./utils/

EXPOSE 5000

CMD ["python", "app.py"]
```

### 3.2 æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t ml-api:v1 .

# è¿è¡Œå®¹å™¨
docker run -d -p 5000:5000 --name ml-service ml-api:v1

# æµ‹è¯•
curl -X POST -F "file=@test.jpg" http://localhost:5000/predict
```

---

## 4. æ¨¡å‹ä¼˜åŒ–

### 4.1 é‡åŒ–

```python
import torch

# åŠ¨æ€é‡åŒ–
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

torch.save(model_quantized.state_dict(), 'model_quantized.pth')

# æ¨¡å‹å¤§å°å¯¹æ¯”
import os
print(f"åŸå§‹æ¨¡å‹: {os.path.getsize('model.pth') / 1024 / 1024:.2f} MB")
print(f"é‡åŒ–æ¨¡å‹: {os.path.getsize('model_quantized.pth') / 1024 / 1024:.2f} MB")
```

---

## å‚è€ƒèµ„æº

- ONNXå®˜æ–¹æ–‡æ¡£
- FastAPIæ–‡æ¡£
- Dockerå®˜æ–¹æ–‡æ¡£

