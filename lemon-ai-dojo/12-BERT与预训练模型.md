# BERTä¸é¢„è®­ç»ƒæ¨¡å‹

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **BERTé€šè¿‡åŒå‘Transformerå­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤º**
2. **é¢„è®­ç»ƒ+å¾®è°ƒæ˜¯NLPçš„æ–°èŒƒå¼**
3. **Masked Language Modelå’ŒNext Sentence Predictionæ˜¯BERTçš„é¢„è®­ç»ƒä»»åŠ¡**
4. **BERTå¯ä»¥ç”¨äºåˆ†ç±»ã€NERã€é—®ç­”ç­‰å¤šç§ä»»åŠ¡**
5. **RoBERTaã€ALBERTã€ELECTRAæ˜¯BERTçš„æ”¹è¿›ç‰ˆæœ¬**

---

## 1. BERTæ¶æ„

### 1.1 åŸç†

```
é¢„è®­ç»ƒä»»åŠ¡ï¼š
1. Masked Language Model (MLM)
   éšæœºé®ç›–15%çš„è¯ï¼Œé¢„æµ‹è¢«é®ç›–çš„è¯
   
2. Next Sentence Prediction (NSP)
   é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­

è¾“å…¥ï¼š[CLS] Sentence A [SEP] Sentence B [SEP]
```

### 1.2 ä½¿ç”¨Hugging Face

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = 'bert-base-chinese'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=2
)

# æ–‡æœ¬åˆ†ç±»
text = "è¿™éƒ¨ç”µå½±å¾ˆç²¾å½©"
inputs = tokenizer(
    text, 
    return_tensors='pt',
    padding=True,
    truncation=True,
    max_length=128
)

# é¢„æµ‹
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    probs = torch.softmax(logits, dim=1)
    pred = torch.argmax(probs, dim=1)
    
print(f"é¢„æµ‹: {pred.item()}")
print(f"æ¦‚ç‡: {probs}")
```

---

## 2. å¾®è°ƒBERT

```python
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset('imdb')

# åˆ†è¯
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# åŠ è½½æ¨¡å‹
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True
)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

trainer.train()
```

---

## 3. å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

```python
from transformers import BertForTokenClassification, BertTokenizerFast

# åŠ è½½æ¨¡å‹
model = BertForTokenClassification.from_pretrained(
    'bert-base-chinese',
    num_labels=7  # O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG
)
tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')

# é¢„æµ‹
text = "è‹¹æœå…¬å¸çš„CEOæ˜¯è’‚å§†Â·åº“å…‹"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=2)

# è§£ç 
label_map = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-LOC', 4: 'I-LOC', 5: 'B-ORG', 6: 'I-ORG'}
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
labels = [label_map[p.item()] for p in predictions[0]]

for token, label in zip(tokens, labels):
    print(f"{token}\t{label}")
```

---

## å‚è€ƒèµ„æº

- Hugging Face Transformersæ–‡æ¡£
- BERTè®ºæ–‡

