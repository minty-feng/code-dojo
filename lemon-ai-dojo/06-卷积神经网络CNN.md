# å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **å·ç§¯å±‚é€šè¿‡å±€éƒ¨è¿æ¥å’Œæƒå€¼å…±äº«å¤§å¹…å‡å°‘å‚æ•°**
2. **æ± åŒ–å±‚é™ä½ç©ºé—´ç»´åº¦ï¼Œå¢å¼ºå¹³ç§»ä¸å˜æ€§**
3. **ç»å…¸æ¶æ„ï¼šLeNet â†’ AlexNet â†’ VGG â†’ ResNet**
4. **æ®‹å·®è¿æ¥è§£å†³æ·±å±‚ç½‘ç»œé€€åŒ–é—®é¢˜**
5. **è¿ç§»å­¦ä¹ å¯ä»¥ç”¨é¢„è®­ç»ƒæ¨¡å‹å¿«é€Ÿè§£å†³æ–°ä»»åŠ¡**

---

## 1. CNNåŸºç¡€

### 1.1 å·ç§¯æ“ä½œ

```
è¾“å…¥ï¼š5Ã—5å›¾åƒ
å·ç§¯æ ¸ï¼š3Ã—3
æ­¥é•¿ï¼š1
è¾“å‡ºï¼š3Ã—3ç‰¹å¾å›¾

å·ç§¯æ ¸æ»‘åŠ¨ï¼Œé€ä¸ªä½ç½®è®¡ç®—å†…ç§¯
```

```python
import torch
import torch.nn as nn

# å·ç§¯å±‚
conv = nn.Conv2d(
    in_channels=1,      # è¾“å…¥é€šé“æ•°
    out_channels=32,    # è¾“å‡ºé€šé“æ•°ï¼ˆå·ç§¯æ ¸æ•°é‡ï¼‰
    kernel_size=3,      # å·ç§¯æ ¸å¤§å°
    stride=1,           # æ­¥é•¿
    padding=1           # å¡«å……
)

# è¾“å…¥ï¼š[batch_size, channels, height, width]
x = torch.randn(1, 1, 28, 28)
output = conv(x)
print(output.shape)  # [1, 32, 28, 28]
```

### 1.2 æ± åŒ–å±‚

```python
# æœ€å¤§æ± åŒ–
pool = nn.MaxPool2d(kernel_size=2, stride=2)

# å¹³å‡æ± åŒ–
pool = nn.AvgPool2d(kernel_size=2, stride=2)

x = torch.randn(1, 32, 28, 28)
output = pool(x)
print(output.shape)  # [1, 32, 14, 14]
```

---

## 2. ç»å…¸CNNæ¶æ„

### 2.1 LeNet-5ï¼ˆ1998ï¼‰

```python
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

### 2.2 AlexNetï¼ˆ2012ï¼‰

```python
class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
```

### 2.3 ResNetï¼ˆ2015ï¼‰

```python
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    
    def forward(self, x):
        residual = x
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual  # æ®‹å·®è¿æ¥
        x = torch.relu(x)
        return x
```

---

## 3. å›¾åƒåˆ†ç±»å®æˆ˜

```python
import torchvision
import torchvision.transforms as transforms

# æ•°æ®å¢å¼º
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# åŠ è½½CIFAR-10
trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform
)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, shuffle=True, num_workers=2
)

# è®­ç»ƒ
model = ResNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(trainloader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}')
```

---

## å‚è€ƒèµ„æº

- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Goodfellow
- PyTorchå®˜æ–¹æ•™ç¨‹
- CS231nè¯¾ç¨‹

