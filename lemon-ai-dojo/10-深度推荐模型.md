# æ·±åº¦æ¨èæ¨¡å‹

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **DeepFMç»“åˆFMå’Œæ·±åº¦ç½‘ç»œï¼Œè‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’**
2. **Wide & DeepåŒæ—¶å­¦ä¹ è®°å¿†å’Œæ³›åŒ–**
3. **DINåŠ¨æ€è®¡ç®—ç”¨æˆ·å…´è¶£ï¼Œé€‚åˆç”µå•†æ¨è**
4. **åŒå¡”æ¨¡å‹æ”¯æŒå‘é‡æ£€ç´¢ï¼Œé€‚åˆå¤§è§„æ¨¡æ¨è**
5. **åºåˆ—æ¨èæ¨¡å‹ï¼ˆå¦‚GRU4Recï¼‰æ•è·ç”¨æˆ·è¡Œä¸ºåºåˆ—**

---

## 1. DeepFM

```python
import torch
import torch.nn as nn

class DeepFM(nn.Module):
    def __init__(self, feature_sizes, embed_dim=8):
        super(DeepFM, self).__init__()
        
        # FMéƒ¨åˆ†ï¼šåµŒå…¥å±‚
        self.embeddings = nn.ModuleList([
            nn.Embedding(size, embed_dim) for size in feature_sizes
        ])
        
        # Deepéƒ¨åˆ†ï¼šDNN
        self.dnn = nn.Sequential(
            nn.Linear(len(feature_sizes) * embed_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 1)
        )
        
        # FMä¸€é˜¶æƒé‡
        self.linear = nn.ModuleList([
            nn.Embedding(size, 1) for size in feature_sizes
        ])
    
    def forward(self, x):
        # FMä¸€é˜¶éƒ¨åˆ†
        linear_part = sum([emb(x[:, i]).squeeze(1) 
                          for i, emb in enumerate(self.linear)])
        
        # FMäºŒé˜¶éƒ¨åˆ†
        embeddings = [emb(x[:, i]) for i, emb in enumerate(self.embeddings)]
        embeddings = torch.stack(embeddings, dim=1)
        
        square_of_sum = torch.sum(embeddings, dim=1) ** 2
        sum_of_square = torch.sum(embeddings ** 2, dim=1)
        fm_part = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)
        
        # Deepéƒ¨åˆ†
        deep_input = embeddings.view(x.size(0), -1)
        deep_part = self.dnn(deep_input)
        
        # ç»„åˆ
        output = linear_part + fm_part + deep_part
        return torch.sigmoid(output)
```

---

## 2. Wide & Deep

```python
class WideAndDeep(nn.Module):
    def __init__(self, wide_dim, deep_dim):
        super(WideAndDeep, self).__init__()
        
        # Wideéƒ¨åˆ†ï¼šçº¿æ€§æ¨¡å‹
        self.wide = nn.Linear(wide_dim, 1)
        
        # Deepéƒ¨åˆ†ï¼šDNN
        self.deep = nn.Sequential(
            nn.Linear(deep_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, wide_input, deep_input):
        wide_out = self.wide(wide_input)
        deep_out = self.deep(deep_input)
        output = wide_out + deep_out
        return torch.sigmoid(output)
```

---

## 3. åºåˆ—æ¨è

```python
class GRU4Rec(nn.Module):
    def __init__(self, num_items, embedding_dim, hidden_size):
        super(GRU4Rec, self).__init__()
        self.embedding = nn.Embedding(num_items, embedding_dim)
        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_items)
    
    def forward(self, item_seq):
        # item_seq: [batch_size, seq_len]
        embedded = self.embedding(item_seq)
        output, hidden = self.gru(embedded)
        logits = self.fc(output[:, -1, :])
        return logits
```

---

## å‚è€ƒèµ„æº

- ã€Šæ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿã€‹- ç‹å–†
- RecBoleæ¨èç³»ç»Ÿåº“

