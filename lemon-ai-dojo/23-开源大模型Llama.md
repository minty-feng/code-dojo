# å¼€æºå¤§æ¨¡å‹Llama

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **Llamaæ˜¯Metaå¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äº†å¼€æºAIå‘å±•**
2. **Llama 2æ”¯æŒå•†ç”¨ï¼Œæœ‰7Bã€13Bã€70Bä¸åŒè§„æ¨¡**
3. **æœ¬åœ°éƒ¨ç½²Llamaå¯ä»¥ä¿æŠ¤æ•°æ®éšç§**
4. **é‡åŒ–æŠ€æœ¯ï¼ˆGPTQã€GGMLï¼‰è®©æ™®é€šç”µè„‘ä¹Ÿèƒ½è¿è¡Œå¤§æ¨¡å‹**
5. **Alpacaã€Vicunaç­‰æ˜¯åŸºäºLlamaçš„å¾®è°ƒæ¨¡å‹**

---

## 1. Llamaç³»åˆ—

### 1.1 å‘å±•å†ç¨‹

```
2023.02  Llama 1    7B-65B    ç ”ç©¶ç”¨é€”
2023.07  Llama 2    7B-70B    å•†ç”¨è®¸å¯
2024.04  Llama 3    8B-70B    æ€§èƒ½å¤§å¹…æå‡
```

### 1.2 æ¨¡å‹è§„æ¨¡

| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|----------|
| Llama-7B | 7B | 14GB | ä¸ªäººç”µè„‘ |
| Llama-13B | 13B | 26GB | å·¥ä½œç«™ |
| Llama-70B | 70B | 140GB | æœåŠ¡å™¨ |

---

## 2. æœ¬åœ°éƒ¨ç½²

### 2.1 ä½¿ç”¨llama.cpp

```bash
# å®‰è£…llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# ä¸‹è½½æ¨¡å‹ï¼ˆé‡åŒ–ç‰ˆæœ¬ï¼‰
wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf

# è¿è¡Œ
./main -m llama-2-7b.Q4_K_M.gguf -p "What is machine learning?" -n 128
```

### 2.2 ä½¿ç”¨Ollama

```bash
# å®‰è£…Ollama
curl https://ollama.ai/install.sh | sh

# ä¸‹è½½å¹¶è¿è¡ŒLlama 2
ollama run llama2

# Pythonè°ƒç”¨
pip install ollama

import ollama

response = ollama.chat(model='llama2', messages=[
  {
    'role': 'user',
    'content': 'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
  },
])
print(response['message']['content'])
```

---

## 3. å¾®è°ƒLlama

### 3.1 ä½¿ç”¨LoRA

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# LoRAé…ç½®
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# å‡†å¤‡æ¨¡å‹
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# è®­ç»ƒ
# ...
```

---

## 4. åº”ç”¨æ¡ˆä¾‹

### 4.1 æœ¬åœ°èŠå¤©æœºå™¨äºº

```python
from transformers import pipeline

# åŠ è½½æœ¬åœ°æ¨¡å‹
chatbot = pipeline(
    "text-generation",
    model="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto"
)

# å¯¹è¯
prompt = """[INST] <<SYS>>
ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹
<</SYS>>

ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ[/INST]"""

response = chatbot(prompt, max_new_tokens=200)
print(response[0]['generated_text'])
```

---

## å‚è€ƒèµ„æº

- Llamaå®˜æ–¹ä»“åº“
- llama.cppé¡¹ç›®
- Ollamaæ–‡æ¡£

