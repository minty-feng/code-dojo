# AI伦理与安全

## 💡 核心结论

1. **AI模型可能存在偏见，需要公平性评估**
2. **对抗样本可以欺骗模型，需要鲁棒性测试**
3. **隐私保护通过差分隐私、联邦学习等技术**
4. **AI生成内容需要水印和检测机制**
5. **负责任的AI开发需要透明性和可解释性**

---

## 1. AI偏见

### 1.1 常见偏见类型

```
数据偏见：
- 历史偏见：训练数据反映历史歧视
- 采样偏见：数据不具代表性

算法偏见：
- 特征选择偏见
- 评估指标偏见
```

### 1.2 检测偏见

```python
# 检查性别偏见
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric

# 计算不同群体的结果差异
metric = BinaryLabelDatasetMetric(dataset, 
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}]
)

print(f"差异影响: {metric.disparate_impact()}")
```

---

## 2. 对抗攻击

### 2.1 FGSM攻击

```python
def fgsm_attack(image, epsilon, data_grad):
    # 获取梯度符号
    sign_data_grad = data_grad.sign()
    # 添加扰动
    perturbed_image = image + epsilon * sign_data_grad
    # 裁剪到有效范围
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    return perturbed_image

# 生成对抗样本
image.requires_grad = True
output = model(image)
loss = criterion(output, target)
model.zero_grad()
loss.backward()

# 添加扰动
perturbed_data = fgsm_attack(image, epsilon=0.1, data_grad=image.grad.data)

# 验证攻击效果
output_after = model(perturbed_data)
```

---

## 3. 模型可解释性

### 3.1 SHAP

```python
import shap

# 加载模型
model = xgboost.XGBClassifier()
model.fit(X_train, y_train)

# SHAP解释器
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 可视化
shap.summary_plot(shap_values, X_test)
```

---

## 参考资源

- AI Fairness 360
- SHAP库文档
- 《Responsible AI》

