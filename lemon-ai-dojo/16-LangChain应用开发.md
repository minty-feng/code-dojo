# LangChainåº”ç”¨å¼€å‘

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **LangChainæ˜¯æ„å»ºLLMåº”ç”¨çš„æ¡†æ¶**
2. **Chainç»„åˆå¤šä¸ªç»„ä»¶å®ç°å¤æ‚å·¥ä½œæµ**
3. **Agentå¯ä»¥ä½¿ç”¨å·¥å…·ï¼Œè‡ªä¸»å†³ç­–è¡ŒåŠ¨**
4. **å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æœ¬åµŒå…¥ï¼Œå®ç°è¯­ä¹‰æœç´¢**
5. **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰è®©LLMè®¿é—®å¤–éƒ¨çŸ¥è¯†**

---

## 1. LangChainæ ¸å¿ƒæ¦‚å¿µ

### 1.1 Models

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# LLM
llm = OpenAI(temperature=0.7, model_name="text-davinci-003")
response = llm("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")

# Chat Model
chat_model = ChatOpenAI(model_name="gpt-3.5-turbo")
from langchain.schema import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶"),
    HumanMessage(content="å¦‚ä½•è¯»å–JSONæ–‡ä»¶ï¼Ÿ")
]
response = chat_model(messages)
```

### 1.2 Prompts

```python
from langchain import PromptTemplate

# å•å˜é‡æ¨¡æ¿
template = "è¯·ç”¨{language}å†™ä¸€ä¸ª{task}çš„ç¤ºä¾‹ä»£ç "
prompt = PromptTemplate(
    input_variables=["language", "task"],
    template=template
)

formatted_prompt = prompt.format(language="Python", task="å¿«é€Ÿæ’åº")

# èŠå¤©æç¤ºæ¨¡æ¿
from langchain.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}"),
    ("human", "{input}")
])

messages = chat_template.format_messages(
    role="ä¸“ä¸šçš„ç¿»è¯‘",
    input="å°†'Hello'ç¿»è¯‘æˆä¸­æ–‡"
)
```

---

## 2. Chains

### 2.1 Simple Chain

```python
from langchain import LLMChain

llm = OpenAI(temperature=0.7)
template = "è¯·ä¸º{product}å†™ä¸€å¥å¹¿å‘Šè¯­"
prompt = PromptTemplate(template=template, input_variables=["product"])

chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run("æ™ºèƒ½æ‰‹è¡¨")
print(result)
```

### 2.2 Sequential Chain

```python
from langchain.chains import SimpleSequentialChain

# Chain 1: ç”Ÿæˆæ•…äº‹
chain1 = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        template="ä¸º{topic}å†™ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹",
        input_variables=["topic"]
    )
)

# Chain 2: æ€»ç»“æ•…äº‹
chain2 = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        template="ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ•…äº‹ï¼š\n{story}",
        input_variables=["story"]
    )
)

# ç»„åˆ
overall_chain = SimpleSequentialChain(chains=[chain1, chain2])
result = overall_chain.run("äººå·¥æ™ºèƒ½")
```

---

## 3. RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

### 3.1 å‘é‡æ•°æ®åº“

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# åŠ è½½æ–‡æ¡£
loader = TextLoader('document.txt')
documents = loader.load()

# åˆ†å‰²æ–‡æ¡£
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = text_splitter.split_documents(documents)

# åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings)

# ç›¸ä¼¼åº¦æœç´¢
query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
similar_docs = vectorstore.similarity_search(query, k=3)

for doc in similar_docs:
    print(doc.page_content)
    print('---')
```

### 3.2 é—®ç­”ç³»ç»Ÿ

```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

question = "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"
answer = qa_chain.run(question)
print(answer)
```

---

## 4. Agents

### 4.1 å·¥å…·

```python
from langchain.agents import Tool, initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun

# å®šä¹‰å·¥å…·
search = DuckDuckGoSearchRun()

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="æœç´¢äº’è”ç½‘ä¿¡æ¯"
    ),
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"
    )
]

# åˆ›å»ºAgent
agent = initialize_agent(
    tools,
    ChatOpenAI(temperature=0),
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# è¿è¡Œ
result = agent.run("2023å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–è·å¾—è€…æ˜¯è°ï¼Ÿä»–ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### 4.2 è‡ªå®šä¹‰å·¥å…·

```python
from langchain.tools import BaseTool

class WeatherTool(BaseTool):
    name = "Weather"
    description = "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"
    
    def _run(self, city: str) -> str:
        # å®é™…è°ƒç”¨å¤©æ°”API
        return f"{city}çš„å¤©æ°”ï¼šæ™´å¤©ï¼Œ25Â°C"
    
    async def _arun(self, city: str) -> str:
        raise NotImplementedError()

tools = [WeatherTool()]
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

result = agent.run("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
```

---

## 5. å®æˆ˜é¡¹ç›®

### 5.1 æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# åŠ è½½PDF
loader = PyPDFLoader("document.pdf")
pages = loader.load_and_split()

# åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(pages, embeddings)

# åˆ›å»ºé—®ç­”é“¾ï¼ˆå¸¦è®°å¿†ï¼‰
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    ChatOpenAI(temperature=0),
    vectorstore.as_retriever(),
    memory=memory
)

# å¤šè½®å¯¹è¯
print(qa_chain({"question": "æ–‡æ¡£ä¸»è¦è®²ä»€ä¹ˆï¼Ÿ"}))
print(qa_chain({"question": "èƒ½è¯¦ç»†è§£é‡Šç¬¬ä¸€éƒ¨åˆ†å—ï¼Ÿ"}))
```

---

## å‚è€ƒèµ„æº

- LangChainå®˜æ–¹æ–‡æ¡£
- LangChain Cookbook

