# Transformeræ¶æ„

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **Transformerå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠ›å¼ƒäº†RNN**
2. **è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—ï¼Œè®­ç»ƒé€Ÿåº¦å¿«**
3. **å¤šå¤´æ³¨æ„åŠ›ä»å¤šä¸ªå­ç©ºé—´å­¦ä¹ è¡¨ç¤º**
4. **ä½ç½®ç¼–ç è®©æ¨¡å‹æ„ŸçŸ¥åºåˆ—é¡ºåº**
5. **Transformeræ˜¯GPTã€BERTç­‰æ¨¡å‹çš„åŸºç¡€**

---

## 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶

### 1.1 åŸç†

```
Query, Key, ValueçŸ©é˜µ

Attention(Q, K, V) = softmax(QKáµ€ / âˆšd_k) * V

æ­¥éª¤ï¼š
1. è®¡ç®—ç›¸ä¼¼åº¦ï¼šQKáµ€
2. ç¼©æ”¾ï¼šé™¤ä»¥âˆšd_kï¼ˆé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼‰
3. softmaxï¼šå¾—åˆ°æ³¨æ„åŠ›æƒé‡
4. åŠ æƒæ±‚å’Œï¼šä¹˜ä»¥V
```

### 1.2 å®ç°

```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        assert self.head_dim * heads == embed_size
        
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
    
    def forward(self, values, keys, query, mask=None):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
        
        # åˆ†æˆå¤šä¸ªå¤´
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)
        
        # Q, K, Vå˜æ¢
        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])
        # energy shape: (N, heads, query_len, key_len)
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))
        
        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)
        
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values])
        out = out.reshape(N, query_len, self.heads * self.head_dim)
        out = self.fc_out(out)
        return out
```

---

## 2. Transformeræ¶æ„

### 2.1 ç¼–ç å™¨

```python
class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        self.attention = SelfAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)
        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
```

### 2.2 ä½ç½®ç¼–ç 

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x
```

---

## 3. ä½¿ç”¨Hugging Face

```python
from transformers import BertTokenizer, BertModel

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# ç¼–ç æ–‡æœ¬
text = "Hello, how are you?"
encoded = tokenizer(text, return_tensors='pt')

# å‰å‘ä¼ æ’­
with torch.no_grad():
    output = model(**encoded)
    last_hidden_states = output.last_hidden_state
```

---

## å‚è€ƒèµ„æº

- ã€ŠAttention is All You Needã€‹è®ºæ–‡
- The Illustrated Transformer
- Hugging Face Transformers

