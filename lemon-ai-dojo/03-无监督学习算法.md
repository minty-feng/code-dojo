# æ— ç›‘ç£å­¦ä¹ ç®—æ³•

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **K-meansé€šè¿‡è¿­ä»£ä¼˜åŒ–èšç±»ä¸­å¿ƒï¼Œéœ€è¦é¢„å…ˆæŒ‡å®šKå€¼**
2. **å±‚æ¬¡èšç±»ä¸éœ€è¦é¢„è®¾Kå€¼ï¼Œç”Ÿæˆæ ‘çŠ¶å›¾**
3. **PCAé€šè¿‡ç‰¹å¾å€¼åˆ†è§£é™ç»´ï¼Œä¿ç•™æœ€å¤§æ–¹å·®**
4. **t-SNEé€‚åˆé«˜ç»´æ•°æ®å¯è§†åŒ–ï¼Œä½†è®¡ç®—é‡å¤§**
5. **å¼‚å¸¸æ£€æµ‹å¯ç”¨Isolation Forestæˆ–One-Class SVM**

---

## 1. èšç±»ç®—æ³•

### 1.1 K-means

```python
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# æ•°æ®
X = np.random.randn(300, 2)

# è®­ç»ƒ
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# èšç±»ä¸­å¿ƒ
centers = kmeans.cluster_centers_

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200)
plt.title('K-meansèšç±»')
plt.show()

# è‚˜éƒ¨æ³•åˆ™é€‰æ‹©Kå€¼
inertias = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, 'bo-')
plt.xlabel('Kå€¼')
plt.ylabel('æƒ¯æ€§')
plt.title('è‚˜éƒ¨æ³•åˆ™')
plt.show()
```

### 1.2 å±‚æ¬¡èšç±»

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# è®­ç»ƒ
model = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = model.fit_predict(X)

# æ ‘çŠ¶å›¾
linkage_matrix = linkage(X, method='ward')
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix)
plt.title('å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾')
plt.show()
```

### 1.3 DBSCAN

```python
from sklearn.cluster import DBSCAN

# åŸºäºå¯†åº¦çš„èšç±»
model = DBSCAN(eps=0.5, min_samples=5)
labels = model.fit_predict(X)

# -1è¡¨ç¤ºå™ªå£°ç‚¹
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"èšç±»æ•°: {n_clusters}")
print(f"å™ªå£°ç‚¹: {n_noise}")
```

---

## 2. é™ç»´ç®—æ³•

### 2.1 PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰

```python
from sklearn.decomposition import PCA

# é™åˆ°2ç»´
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# æ–¹å·®è§£é‡Šç‡
print(f"æ–¹å·®è§£é‡Šç‡: {pca.explained_variance_ratio_}")
print(f"ç´¯è®¡æ–¹å·®: {pca.explained_variance_ratio_.cumsum()}")

# è‡ªåŠ¨é€‰æ‹©ç»´åº¦ï¼ˆä¿ç•™95%æ–¹å·®ï¼‰
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X)
print(f"é™ç»´åç»´åº¦: {X_reduced.shape[1]}")
```

### 2.2 t-SNE

```python
from sklearn.manifold import TSNE

# é™åˆ°2ç»´å¯è§†åŒ–
tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X)

plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='viridis')
plt.title('t-SNEå¯è§†åŒ–')
plt.show()
```

---

## 3. å¼‚å¸¸æ£€æµ‹

```python
from sklearn.ensemble import IsolationForest

# è®­ç»ƒ
model = IsolationForest(contamination=0.1, random_state=42)
predictions = model.fit_predict(X)

# -1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸
anomalies = X[predictions == -1]
normal = X[predictions == 1]

plt.scatter(normal[:, 0], normal[:, 1], c='blue', label='æ­£å¸¸')
plt.scatter(anomalies[:, 0], anomalies[:, 1], c='red', label='å¼‚å¸¸')
plt.legend()
plt.show()
```

---

## å‚è€ƒèµ„æº

- Scikit-learn Clustering
- ã€ŠPattern Recognition and Machine Learningã€‹

