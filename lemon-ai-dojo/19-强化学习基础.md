# 强化学习基础

## 💡 核心结论

1. **强化学习通过试错学习最优策略**
2. **马尔可夫决策过程（MDP）是强化学习的数学基础**
3. **Q-learning学习状态-动作价值函数**
4. **策略梯度直接优化策略**
5. **Actor-Critic结合价值和策略方法**

---

## 1. 核心概念

```
Agent（智能体）
  ↓ 选择Action
Environment（环境）
  ↓ 返回State和Reward
Agent学习策略π，最大化累积奖励
```

### 1.1 马尔可夫决策过程

```
状态（State）：s
动作（Action）：a
奖励（Reward）：r
策略（Policy）：π(a|s)
价值函数：V(s) = E[累积奖励|s]
```

---

## 2. Q-Learning

```python
import numpy as np

class QLearning:
    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.9):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = learning_rate
        self.gamma = gamma
    
    def choose_action(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            # 探索
            return np.random.randint(self.q_table.shape[1])
        else:
            # 利用
            return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        # Q-learning更新公式
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - predict)

# 训练
agent = QLearning(n_states=100, n_actions=4)

for episode in range(1000):
    state = env.reset()
    
    while True:
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        
        agent.learn(state, action, reward, next_state)
        
        state = next_state
        if done:
            break
```

---

## 3. Deep Q-Network（DQN）

```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, x):
        return self.fc(x)

# 经验回放
from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

# 训练
model = DQN(state_dim=4, action_dim=2)
target_model = DQN(state_dim=4, action_dim=2)
target_model.load_state_dict(model.state_dict())

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
buffer = ReplayBuffer()

for episode in range(1000):
    state = env.reset()
    
    while True:
        # 选择动作
        if random.random() < epsilon:
            action = random.randint(0, 1)
        else:
            with torch.no_grad():
                q_values = model(torch.FloatTensor(state))
                action = q_values.argmax().item()
        
        # 执行动作
        next_state, reward, done = env.step(action)
        
        # 存储经验
        buffer.push(state, action, reward, next_state, done)
        
        # 训练
        if len(buffer.buffer) > 128:
            batch = buffer.sample(128)
            # ... 训练代码
        
        state = next_state
        if done:
            break
```

---

## 参考资源

- 《Reinforcement Learning: An Introduction》
- OpenAI Gym
- Stable-Baselines3

