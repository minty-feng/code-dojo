# å¼ºåŒ–å­¦ä¹ åŸºç¡€

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥**
2. **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€**
3. **Q-learningå­¦ä¹ çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°**
4. **ç­–ç•¥æ¢¯åº¦ç›´æ¥ä¼˜åŒ–ç­–ç•¥**
5. **Actor-Criticç»“åˆä»·å€¼å’Œç­–ç•¥æ–¹æ³•**

---

## 1. æ ¸å¿ƒæ¦‚å¿µ

```
Agentï¼ˆæ™ºèƒ½ä½“ï¼‰
  â†“ é€‰æ‹©Action
Environmentï¼ˆç¯å¢ƒï¼‰
  â†“ è¿”å›Stateå’ŒReward
Agentå­¦ä¹ ç­–ç•¥Ï€ï¼Œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±
```

### 1.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

```
çŠ¶æ€ï¼ˆStateï¼‰ï¼šs
åŠ¨ä½œï¼ˆActionï¼‰ï¼ša
å¥–åŠ±ï¼ˆRewardï¼‰ï¼šr
ç­–ç•¥ï¼ˆPolicyï¼‰ï¼šÏ€(a|s)
ä»·å€¼å‡½æ•°ï¼šV(s) = E[ç´¯ç§¯å¥–åŠ±|s]
```

---

## 2. Q-Learning

```python
import numpy as np

class QLearning:
    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.9):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = learning_rate
        self.gamma = gamma
    
    def choose_action(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            # æ¢ç´¢
            return np.random.randint(self.q_table.shape[1])
        else:
            # åˆ©ç”¨
            return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        # Q-learningæ›´æ–°å…¬å¼
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - predict)

# è®­ç»ƒ
agent = QLearning(n_states=100, n_actions=4)

for episode in range(1000):
    state = env.reset()
    
    while True:
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        
        agent.learn(state, action, reward, next_state)
        
        state = next_state
        if done:
            break
```

---

## 3. Deep Q-Networkï¼ˆDQNï¼‰

```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, x):
        return self.fc(x)

# ç»éªŒå›æ”¾
from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

# è®­ç»ƒ
model = DQN(state_dim=4, action_dim=2)
target_model = DQN(state_dim=4, action_dim=2)
target_model.load_state_dict(model.state_dict())

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
buffer = ReplayBuffer()

for episode in range(1000):
    state = env.reset()
    
    while True:
        # é€‰æ‹©åŠ¨ä½œ
        if random.random() < epsilon:
            action = random.randint(0, 1)
        else:
            with torch.no_grad():
                q_values = model(torch.FloatTensor(state))
                action = q_values.argmax().item()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done = env.step(action)
        
        # å­˜å‚¨ç»éªŒ
        buffer.push(state, action, reward, next_state, done)
        
        # è®­ç»ƒ
        if len(buffer.buffer) > 128:
            batch = buffer.sample(128)
            # ... è®­ç»ƒä»£ç 
        
        state = next_state
        if done:
            break
```

---

## å‚è€ƒèµ„æº

- ã€ŠReinforcement Learning: An Introductionã€‹
- OpenAI Gym
- Stable-Baselines3

