# å‘é‡æ•°æ®åº“ä¸RAG

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **å‘é‡æ•°æ®åº“å­˜å‚¨é«˜ç»´å‘é‡ï¼Œæ”¯æŒé«˜æ•ˆç›¸ä¼¼åº¦æœç´¢**
2. **RAGé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¢å¼ºLLMç”Ÿæˆèƒ½åŠ›**
3. **Embeddingæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º**
4. **åˆ†å—ç­–ç•¥å½±å“æ£€ç´¢è´¨é‡ï¼Œéœ€è¦å¹³è¡¡ç²’åº¦å’Œè¯­ä¹‰**
5. **æ··åˆæ£€ç´¢ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æœç´¢ï¼Œæ•ˆæœæ›´å¥½**

---

## 1. å‘é‡æ•°æ®åº“

### 1.1 ä¸»æµå‘é‡æ•°æ®åº“

| æ•°æ®åº“ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|----------|
| Chroma | è½»é‡ã€æ˜“ç”¨ | åŸå‹å¼€å‘ |
| Pinecone | äº‘æœåŠ¡ã€é«˜æ€§èƒ½ | ç”Ÿäº§ç¯å¢ƒ |
| Milvus | å¼€æºã€åˆ†å¸ƒå¼ | å¤§è§„æ¨¡åº”ç”¨ |
| Weaviate | æ··åˆæœç´¢ | ä¼ä¸šåº”ç”¨ |
| Qdrant | é«˜æ€§èƒ½ã€Rust | å®æ—¶æœç´¢ |

### 1.2 ä½¿ç”¨Chroma

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter

# æ–‡æ¡£
documents = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ",
    "Transformeræ˜¯NLPçš„çªç ´æ€§æ¶æ„"
]

# åˆ†å‰²æ–‡æœ¬
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
docs = text_splitter.create_documents(documents)

# åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# ç›¸ä¼¼åº¦æœç´¢
query = "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"
results = vectorstore.similarity_search(query, k=2)

for doc in results:
    print(doc.page_content)
    print('---')

# å¸¦åˆ†æ•°çš„æœç´¢
results_with_scores = vectorstore.similarity_search_with_score(query, k=2)

for doc, score in results_with_scores:
    print(f"åˆ†æ•°: {score}")
    print(doc.page_content)
    print('---')
```

---

## 2. RAGå®ç°

### 2.1 åŸºç¡€RAG

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# åˆ›å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# æé—®
query = "Transformerçš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿ"
result = qa_chain({"query": query})

print("å›ç­”:", result['result'])
print("\næ¥æºæ–‡æ¡£:")
for doc in result['source_documents']:
    print(doc.page_content)
```

### 2.2 é«˜çº§RAG

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# å‹ç¼©æ£€ç´¢å™¨ï¼ˆè¿‡æ»¤æ— å…³å†…å®¹ï¼‰
llm = ChatOpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)

# æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + è¯­ä¹‰ï¼‰
from langchain.retrievers import BM25Retriever, EnsembleRetriever

bm25_retriever = BM25Retriever.from_documents(docs)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vectorstore.as_retriever()],
    weights=[0.5, 0.5]
)

# ä½¿ç”¨
results = ensemble_retriever.get_relevant_documents("Transformer")
```

---

## 3. æ–‡æ¡£å¤„ç†

### 3.1 æ–‡æ¡£åŠ è½½

```python
from langchain.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    TextLoader,
    WebBaseLoader
)

# PDF
loader = PyPDFLoader("document.pdf")
pages = loader.load_and_split()

# Markdown
loader = UnstructuredMarkdownLoader("README.md")
docs = loader.load()

# ç½‘é¡µ
loader = WebBaseLoader("https://example.com/article")
docs = loader.load()
```

### 3.2 æ–‡æœ¬åˆ†å—ç­–ç•¥

```python
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)

# æŒ‰å­—ç¬¦åˆ†å‰²
splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separator="\n\n"
)

# é€’å½’åˆ†å‰²ï¼ˆæ¨èï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)

# æŒ‰tokenåˆ†å‰²
splitter = TokenTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

---

## 4. ç”Ÿäº§çº§RAG

```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# è®°å¿†
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key='answer'
)

# é—®ç­”é“¾
qa = ConversationalRetrievalChain.from_llm(
    ChatOpenAI(temperature=0.7),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    memory=memory,
    return_source_documents=True
)

# å¤šè½®å¯¹è¯
response1 = qa({"question": "æ–‡æ¡£ä¸»è¦è®²ä»€ä¹ˆï¼Ÿ"})
print(response1['answer'])

response2 = qa({"question": "èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹ç¬¬ä¸€éƒ¨åˆ†å—ï¼Ÿ"})
print(response2['answer'])
```

---

## å‚è€ƒèµ„æº

- LangChainæ–‡æ¡£
- Chromaæ–‡æ¡£
- RAG Surveyè®ºæ–‡

