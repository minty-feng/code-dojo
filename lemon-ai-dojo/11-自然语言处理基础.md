# è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **æ–‡æœ¬éœ€è¦è½¬æ¢ä¸ºå‘é‡æ‰èƒ½è¢«æ¨¡å‹å¤„ç†**
2. **è¯è¢‹æ¨¡å‹ç®€å•ä½†ä¸¢å¤±é¡ºåºï¼ŒTF-IDFè€ƒè™‘è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡**
3. **Word2Vecå­¦ä¹ è¯å‘é‡ï¼Œç›¸ä¼¼è¯è·ç¦»è¿‘**
4. **RNN/LSTMå¤„ç†åºåˆ—ï¼ŒTransformeræˆä¸ºä¸»æµ**
5. **é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTã€GPTï¼‰æ˜¯NLPçš„èŒƒå¼è½¬å˜**

---

## 1. æ–‡æœ¬é¢„å¤„ç†

```python
import re
import jieba  # ä¸­æ–‡åˆ†è¯

# æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    # è½¬å°å†™
    text = text.lower()
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ä¸­æ–‡åˆ†è¯
text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
words = jieba.cut(text)
print(' '.join(words))  # æˆ‘ çˆ± è‡ªç„¶è¯­è¨€ å¤„ç†

# åœç”¨è¯è¿‡æ»¤
stopwords = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨'])
words = [w for w in words if w not in stopwords]
```

---

## 2. æ–‡æœ¬è¡¨ç¤º

### 2.1 è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'This is the first document',
    'This is the second document',
    'And this is the third one'
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

### 2.2 TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print(X.toarray())

# TF-IDFå…¬å¼ï¼š
# TF(t, d) = è¯tåœ¨æ–‡æ¡£dä¸­çš„é¢‘ç‡
# IDF(t) = log(æ–‡æ¡£æ€»æ•° / åŒ…å«è¯tçš„æ–‡æ¡£æ•°)
# TF-IDF(t, d) = TF(t, d) * IDF(t)
```

---

## 3. æ–‡æœ¬åˆ†ç±»

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# æ•°æ®
texts = ['å¾ˆå¥½çš„ç”µå½±', 'éå¸¸å·®çš„ä½“éªŒ', 'è¶…çº§æ£’', 'å¤ªçƒ‚äº†']
labels = [1, 0, 1, 0]  # 1:æ­£é¢ï¼Œ0:è´Ÿé¢

# å‘é‡åŒ–
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# è®­ç»ƒ
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)
model = MultinomialNB()
model.fit(X_train, y_train)

# é¢„æµ‹
new_text = ['è¿™éƒ¨ç”µå½±å¾ˆç²¾å½©']
X_new = vectorizer.transform(new_text)
pred = model.predict(X_new)
proba = model.predict_proba(X_new)
```

---

## 4. Word2Vec

```python
from gensim.models import Word2Vec

# è®­ç»ƒæ•°æ®
sentences = [
    ['æˆ‘', 'çˆ±', 'æœºå™¨å­¦ä¹ '],
    ['æ·±åº¦å­¦ä¹ ', 'å¾ˆ', 'æœ‰è¶£'],
    ['è‡ªç„¶è¯­è¨€', 'å¤„ç†', 'å¾ˆ', 'é‡è¦']
]

# è®­ç»ƒ
model = Word2Vec(
    sentences, 
    vector_size=100,  # è¯å‘é‡ç»´åº¦
    window=5,         # ä¸Šä¸‹æ–‡çª—å£
    min_count=1,      # æœ€å°è¯é¢‘
    workers=4
)

# è·å–è¯å‘é‡
vector = model.wv['æœºå™¨å­¦ä¹ ']

# ç›¸ä¼¼è¯
similar_words = model.wv.most_similar('æœºå™¨å­¦ä¹ ', topn=5)
print(similar_words)

# è¯è¯­è¿ç®—
result = model.wv.most_similar(
    positive=['å›½ç‹', 'å¥³äºº'],
    negative=['ç”·äºº']
)  # ç»“æœæ¥è¿‘"å¥³ç‹"
```

---

## å‚è€ƒèµ„æº

- ã€ŠPythonè‡ªç„¶è¯­è¨€å¤„ç†ã€‹
- NLTKã€spaCyåº“
- Gensimæ–‡æ¡£

