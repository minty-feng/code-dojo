# 自然语言处理基础

## 💡 核心结论

1. **文本需要转换为向量才能被模型处理**
2. **词袋模型简单但丢失顺序，TF-IDF考虑词频和逆文档频率**
3. **Word2Vec学习词向量，相似词距离近**
4. **RNN/LSTM处理序列，Transformer成为主流**
5. **预训练模型（BERT、GPT）是NLP的范式转变**

---

## 1. 文本预处理

```python
import re
import jieba  # 中文分词

# 清洗文本
def clean_text(text):
    # 转小写
    text = text.lower()
    # 移除特殊字符
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # 移除多余空格
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 中文分词
text = "我爱自然语言处理"
words = jieba.cut(text)
print(' '.join(words))  # 我 爱 自然语言 处理

# 停用词过滤
stopwords = set(['的', '了', '是', '在'])
words = [w for w in words if w not in stopwords]
```

---

## 2. 文本表示

### 2.1 词袋模型（Bag of Words）

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'This is the first document',
    'This is the second document',
    'And this is the third one'
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

### 2.2 TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print(X.toarray())

# TF-IDF公式：
# TF(t, d) = 词t在文档d中的频率
# IDF(t) = log(文档总数 / 包含词t的文档数)
# TF-IDF(t, d) = TF(t, d) * IDF(t)
```

---

## 3. 文本分类

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# 数据
texts = ['很好的电影', '非常差的体验', '超级棒', '太烂了']
labels = [1, 0, 1, 0]  # 1:正面，0:负面

# 向量化
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# 训练
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)
model = MultinomialNB()
model.fit(X_train, y_train)

# 预测
new_text = ['这部电影很精彩']
X_new = vectorizer.transform(new_text)
pred = model.predict(X_new)
proba = model.predict_proba(X_new)
```

---

## 4. Word2Vec

```python
from gensim.models import Word2Vec

# 训练数据
sentences = [
    ['我', '爱', '机器学习'],
    ['深度学习', '很', '有趣'],
    ['自然语言', '处理', '很', '重要']
]

# 训练
model = Word2Vec(
    sentences, 
    vector_size=100,  # 词向量维度
    window=5,         # 上下文窗口
    min_count=1,      # 最小词频
    workers=4
)

# 获取词向量
vector = model.wv['机器学习']

# 相似词
similar_words = model.wv.most_similar('机器学习', topn=5)
print(similar_words)

# 词语运算
result = model.wv.most_similar(
    positive=['国王', '女人'],
    negative=['男人']
)  # 结果接近"女王"
```

---

## 参考资源

- 《Python自然语言处理》
- NLTK、spaCy库
- Gensim文档

