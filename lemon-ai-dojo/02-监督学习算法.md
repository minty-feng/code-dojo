# 监督学习算法

## 💡 核心结论

1. **线性回归是最简单的回归算法，通过最小二乘法求解**
2. **逻辑回归用于分类问题，输出概率值**
3. **决策树易于理解和解释，但容易过拟合**
4. **随机森林通过集成多棵树降低过拟合**
5. **SVM寻找最大间隔超平面，适合高维数据**

---

## 1. 线性回归

### 1.1 原理

**假设函数**：
```
h(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ
或向量形式：h(x) = θᵀx
```

**损失函数（MSE）**：
```
J(θ) = (1/2m) Σ(h(xⁱ) - yⁱ)²
```

**梯度下降**：
```
θⱼ := θⱼ - α ∂J(θ)/∂θⱼ
```

### 1.2 实现

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# 数据
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# 训练
model = LinearRegression()
model.fit(X, y)

# 参数
print(f"斜率: {model.coef_[0]}")
print(f"截距: {model.intercept_}")

# 预测
X_new = np.array([[6]])
y_pred = model.predict(X_new)
print(f"预测: {y_pred[0]}")

# 手动实现梯度下降
def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
    m = len(y)
    theta = np.zeros(X.shape[1])
    
    for _ in range(iterations):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        theta -= learning_rate * gradient
    
    return theta
```

---

## 2. 逻辑回归

### 2.1 原理

**Sigmoid函数**：
```
σ(z) = 1 / (1 + e⁻ᶻ)
```

**假设函数**：
```
h(x) = σ(θᵀx) = 1 / (1 + e⁻θᵀˣ)
```

**损失函数（交叉熵）**：
```
J(θ) = -(1/m) Σ[yⁱlog(h(xⁱ)) + (1-yⁱ)log(1-h(xⁱ))]
```

### 2.2 实现

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# 生成数据
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 训练
model = LogisticRegression()
model.fit(X_train, y_train)

# 预测概率
proba = model.predict_proba(X_test)
print(f"概率: {proba[0]}")  # [P(类别0), P(类别1)]

# 预测类别
pred = model.predict(X_test)

# 评估
accuracy = model.score(X_test, y_test)
print(f"准确率: {accuracy:.2%}")
```

---

## 3. 决策树

### 3.1 原理

**信息增益**：
```
IG(D, A) = Entropy(D) - Σ(|Dᵥ|/|D|) * Entropy(Dᵥ)

熵：H(D) = -Σ pₖ log₂(pₖ)
```

**基尼指数**：
```
Gini(D) = 1 - Σ pₖ²
```

### 3.2 实现

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 训练
model = DecisionTreeClassifier(
    max_depth=3,           # 最大深度
    min_samples_split=20,  # 最小分裂样本数
    min_samples_leaf=10,   # 叶节点最小样本数
    criterion='gini'       # 'gini' 或 'entropy'
)
model.fit(X_train, y_train)

# 可视化
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=feature_names)
plt.show()

# 特征重要性
importances = model.feature_importances_
for i, imp in enumerate(importances):
    print(f"特征{i}: {imp:.4f}")
```

---

## 4. 随机森林

### 4.1 原理

**Bagging + 随机特征选择**：
```
1. Bootstrap采样：有放回抽样生成多个训练集
2. 训练多棵决策树
3. 每次分裂时随机选择部分特征
4. 投票或平均得到最终结果
```

### 4.2 实现

```python
from sklearn.ensemble import RandomForestClassifier

# 训练
model = RandomForestClassifier(
    n_estimators=100,      # 树的数量
    max_depth=10,          # 最大深度
    min_samples_split=20,
    n_jobs=-1,             # 并行
    random_state=42
)
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)
print(f"准确率: {accuracy:.2%}")

# 特征重要性
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.title('特征重要性')
plt.show()
```

---

## 5. 支持向量机（SVM）

### 5.1 原理

**目标**：找到最大间隔超平面

```
最大化：2/||w||
约束：yⁱ(wᵀxⁱ + b) ≥ 1

等价于最小化：(1/2)||w||²
```

**核技巧**：
- 线性核：K(x, x') = xᵀx'
- 多项式核：K(x, x') = (xᵀx' + c)ᵈ
- RBF核：K(x, x') = exp(-γ||x - x'||²)

### 5.2 实现

```python
from sklearn.svm import SVC

# 线性SVM
model = SVC(kernel='linear', C=1.0)
model.fit(X_train, y_train)

# RBF核SVM
model = SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)

# 支持向量
print(f"支持向量数量: {len(model.support_vectors_)}")
```

---

## 6. K近邻（KNN）

### 6.1 原理

```
1. 计算测试样本与所有训练样本的距离
2. 选择距离最近的K个样本
3. 分类：投票；回归：平均
```

### 6.2 实现

```python
from sklearn.neighbors import KNeighborsClassifier

# 训练（实际上只是存储数据）
model = KNeighborsClassifier(
    n_neighbors=5,        # K值
    weights='distance',   # 'uniform' 或 'distance'
    metric='euclidean'    # 距离度量
)
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 找到最近的邻居
distances, indices = model.kneighbors(X_test[:1])
print(f"最近的5个邻居索引: {indices}")
print(f"距离: {distances}")
```

---

## 7. 朴素贝叶斯

### 7.1 原理

**贝叶斯定理**：
```
P(A|B) = P(B|A) * P(A) / P(B)

P(类别|特征) = P(特征|类别) * P(类别) / P(特征)
```

**朴素假设**：特征之间相互独立

### 7.2 实现

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB

# 高斯朴素贝叶斯（连续特征）
model = GaussianNB()
model.fit(X_train, y_train)

# 多项式朴素贝叶斯（文本分类）
from sklearn.feature_extraction.text import CountVectorizer

texts = ['I love this movie', 'This film is terrible']
labels = [1, 0]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

model = MultinomialNB()
model.fit(X, labels)

# 预测
new_text = ['Great movie']
X_new = vectorizer.transform(new_text)
pred = model.predict(X_new)
```

---

## 8. 梯度提升树（GBDT）

### 8.1 实现

```python
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    random_state=42
)
model.fit(X_train, y_train)

# XGBoost（更快更强）
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
model.fit(X_train, y_train)

# LightGBM（更快）
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
model.fit(X_train, y_train)
```

---

## 9. 算法对比

| 算法 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| 线性回归 | 简单、快速、可解释 | 只能拟合线性关系 | 线性关系明显 |
| 逻辑回归 | 概率输出、可解释 | 线性决策边界 | 二分类问题 |
| 决策树 | 易理解、不需归一化 | 容易过拟合 | 需要可解释性 |
| 随机森林 | 准确、鲁棒、特征重要性 | 训练慢、模型大 | 表格数据 |
| SVM | 高维有效、灵活 | 大数据慢、难调参 | 小样本、高维 |
| KNN | 简单、无训练 | 预测慢、高维差 | 小数据集 |
| XGBoost | 准确、快速 | 需调参、黑盒 | 竞赛、表格数据 |

---

## 参考资源

- 《机器学习》- 周志华
- Scikit-learn官方文档
- Kaggle竞赛

