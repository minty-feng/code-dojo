# ç›‘ç£å­¦ä¹ ç®—æ³•

## ğŸ’¡ æ ¸å¿ƒç»“è®º

1. **çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å›å½’ç®—æ³•ï¼Œé€šè¿‡æœ€å°äºŒä¹˜æ³•æ±‚è§£**
2. **é€»è¾‘å›å½’ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºæ¦‚ç‡å€¼**
3. **å†³ç­–æ ‘æ˜“äºç†è§£å’Œè§£é‡Šï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆ**
4. **éšæœºæ£®æ—é€šè¿‡é›†æˆå¤šæ£µæ ‘é™ä½è¿‡æ‹Ÿåˆ**
5. **SVMå¯»æ‰¾æœ€å¤§é—´éš”è¶…å¹³é¢ï¼Œé€‚åˆé«˜ç»´æ•°æ®**

---

## 1. çº¿æ€§å›å½’

### 1.1 åŸç†

**å‡è®¾å‡½æ•°**ï¼š
```
h(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
æˆ–å‘é‡å½¢å¼ï¼šh(x) = Î¸áµ€x
```

**æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰**ï¼š
```
J(Î¸) = (1/2m) Î£(h(xâ±) - yâ±)Â²
```

**æ¢¯åº¦ä¸‹é™**ï¼š
```
Î¸â±¼ := Î¸â±¼ - Î± âˆ‚J(Î¸)/âˆ‚Î¸â±¼
```

### 1.2 å®ç°

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# æ•°æ®
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# è®­ç»ƒ
model = LinearRegression()
model.fit(X, y)

# å‚æ•°
print(f"æ–œç‡: {model.coef_[0]}")
print(f"æˆªè·: {model.intercept_}")

# é¢„æµ‹
X_new = np.array([[6]])
y_pred = model.predict(X_new)
print(f"é¢„æµ‹: {y_pred[0]}")

# æ‰‹åŠ¨å®ç°æ¢¯åº¦ä¸‹é™
def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
    m = len(y)
    theta = np.zeros(X.shape[1])
    
    for _ in range(iterations):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        theta -= learning_rate * gradient
    
    return theta
```

---

## 2. é€»è¾‘å›å½’

### 2.1 åŸç†

**Sigmoidå‡½æ•°**ï¼š
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)
```

**å‡è®¾å‡½æ•°**ï¼š
```
h(x) = Ïƒ(Î¸áµ€x) = 1 / (1 + eâ»Î¸áµ€Ë£)
```

**æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰**ï¼š
```
J(Î¸) = -(1/m) Î£[yâ±log(h(xâ±)) + (1-yâ±)log(1-h(xâ±))]
```

### 2.2 å®ç°

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# è®­ç»ƒ
model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹æ¦‚ç‡
proba = model.predict_proba(X_test)
print(f"æ¦‚ç‡: {proba[0]}")  # [P(ç±»åˆ«0), P(ç±»åˆ«1)]

# é¢„æµ‹ç±»åˆ«
pred = model.predict(X_test)

# è¯„ä¼°
accuracy = model.score(X_test, y_test)
print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
```

---

## 3. å†³ç­–æ ‘

### 3.1 åŸç†

**ä¿¡æ¯å¢ç›Š**ï¼š
```
IG(D, A) = Entropy(D) - Î£(|Dáµ¥|/|D|) * Entropy(Dáµ¥)

ç†µï¼šH(D) = -Î£ pâ‚– logâ‚‚(pâ‚–)
```

**åŸºå°¼æŒ‡æ•°**ï¼š
```
Gini(D) = 1 - Î£ pâ‚–Â²
```

### 3.2 å®ç°

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# è®­ç»ƒ
model = DecisionTreeClassifier(
    max_depth=3,           # æœ€å¤§æ·±åº¦
    min_samples_split=20,  # æœ€å°åˆ†è£‚æ ·æœ¬æ•°
    min_samples_leaf=10,   # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    criterion='gini'       # 'gini' æˆ– 'entropy'
)
model.fit(X_train, y_train)

# å¯è§†åŒ–
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=feature_names)
plt.show()

# ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
for i, imp in enumerate(importances):
    print(f"ç‰¹å¾{i}: {imp:.4f}")
```

---

## 4. éšæœºæ£®æ—

### 4.1 åŸç†

**Bagging + éšæœºç‰¹å¾é€‰æ‹©**ï¼š
```
1. Bootstrapé‡‡æ ·ï¼šæœ‰æ”¾å›æŠ½æ ·ç”Ÿæˆå¤šä¸ªè®­ç»ƒé›†
2. è®­ç»ƒå¤šæ£µå†³ç­–æ ‘
3. æ¯æ¬¡åˆ†è£‚æ—¶éšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾
4. æŠ•ç¥¨æˆ–å¹³å‡å¾—åˆ°æœ€ç»ˆç»“æœ
```

### 4.2 å®ç°

```python
from sklearn.ensemble import RandomForestClassifier

# è®­ç»ƒ
model = RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=10,          # æœ€å¤§æ·±åº¦
    min_samples_split=20,
    n_jobs=-1,             # å¹¶è¡Œ
    random_state=42
)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)
print(f"å‡†ç¡®ç‡: {accuracy:.2%}")

# ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.title('ç‰¹å¾é‡è¦æ€§')
plt.show()
```

---

## 5. æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

### 5.1 åŸç†

**ç›®æ ‡**ï¼šæ‰¾åˆ°æœ€å¤§é—´éš”è¶…å¹³é¢

```
æœ€å¤§åŒ–ï¼š2/||w||
çº¦æŸï¼šyâ±(wáµ€xâ± + b) â‰¥ 1

ç­‰ä»·äºæœ€å°åŒ–ï¼š(1/2)||w||Â²
```

**æ ¸æŠ€å·§**ï¼š
- çº¿æ€§æ ¸ï¼šK(x, x') = xáµ€x'
- å¤šé¡¹å¼æ ¸ï¼šK(x, x') = (xáµ€x' + c)áµˆ
- RBFæ ¸ï¼šK(x, x') = exp(-Î³||x - x'||Â²)

### 5.2 å®ç°

```python
from sklearn.svm import SVC

# çº¿æ€§SVM
model = SVC(kernel='linear', C=1.0)
model.fit(X_train, y_train)

# RBFæ ¸SVM
model = SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)

# æ”¯æŒå‘é‡
print(f"æ”¯æŒå‘é‡æ•°é‡: {len(model.support_vectors_)}")
```

---

## 6. Kè¿‘é‚»ï¼ˆKNNï¼‰

### 6.1 åŸç†

```
1. è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
2. é€‰æ‹©è·ç¦»æœ€è¿‘çš„Kä¸ªæ ·æœ¬
3. åˆ†ç±»ï¼šæŠ•ç¥¨ï¼›å›å½’ï¼šå¹³å‡
```

### 6.2 å®ç°

```python
from sklearn.neighbors import KNeighborsClassifier

# è®­ç»ƒï¼ˆå®é™…ä¸Šåªæ˜¯å­˜å‚¨æ•°æ®ï¼‰
model = KNeighborsClassifier(
    n_neighbors=5,        # Kå€¼
    weights='distance',   # 'uniform' æˆ– 'distance'
    metric='euclidean'    # è·ç¦»åº¦é‡
)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…
distances, indices = model.kneighbors(X_test[:1])
print(f"æœ€è¿‘çš„5ä¸ªé‚»å±…ç´¢å¼•: {indices}")
print(f"è·ç¦»: {distances}")
```

---

## 7. æœ´ç´ è´å¶æ–¯

### 7.1 åŸç†

**è´å¶æ–¯å®šç†**ï¼š
```
P(A|B) = P(B|A) * P(A) / P(B)

P(ç±»åˆ«|ç‰¹å¾) = P(ç‰¹å¾|ç±»åˆ«) * P(ç±»åˆ«) / P(ç‰¹å¾)
```

**æœ´ç´ å‡è®¾**ï¼šç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹

### 7.2 å®ç°

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB

# é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼ˆè¿ç»­ç‰¹å¾ï¼‰
model = GaussianNB()
model.fit(X_train, y_train)

# å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰
from sklearn.feature_extraction.text import CountVectorizer

texts = ['I love this movie', 'This film is terrible']
labels = [1, 0]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

model = MultinomialNB()
model.fit(X, labels)

# é¢„æµ‹
new_text = ['Great movie']
X_new = vectorizer.transform(new_text)
pred = model.predict(X_new)
```

---

## 8. æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰

### 8.1 å®ç°

```python
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    random_state=42
)
model.fit(X_train, y_train)

# XGBoostï¼ˆæ›´å¿«æ›´å¼ºï¼‰
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
model.fit(X_train, y_train)

# LightGBMï¼ˆæ›´å¿«ï¼‰
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
model.fit(X_train, y_train)
```

---

## 9. ç®—æ³•å¯¹æ¯”

| ç®—æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| çº¿æ€§å›å½’ | ç®€å•ã€å¿«é€Ÿã€å¯è§£é‡Š | åªèƒ½æ‹Ÿåˆçº¿æ€§å…³ç³» | çº¿æ€§å…³ç³»æ˜æ˜¾ |
| é€»è¾‘å›å½’ | æ¦‚ç‡è¾“å‡ºã€å¯è§£é‡Š | çº¿æ€§å†³ç­–è¾¹ç•Œ | äºŒåˆ†ç±»é—®é¢˜ |
| å†³ç­–æ ‘ | æ˜“ç†è§£ã€ä¸éœ€å½’ä¸€åŒ– | å®¹æ˜“è¿‡æ‹Ÿåˆ | éœ€è¦å¯è§£é‡Šæ€§ |
| éšæœºæ£®æ— | å‡†ç¡®ã€é²æ£’ã€ç‰¹å¾é‡è¦æ€§ | è®­ç»ƒæ…¢ã€æ¨¡å‹å¤§ | è¡¨æ ¼æ•°æ® |
| SVM | é«˜ç»´æœ‰æ•ˆã€çµæ´» | å¤§æ•°æ®æ…¢ã€éš¾è°ƒå‚ | å°æ ·æœ¬ã€é«˜ç»´ |
| KNN | ç®€å•ã€æ— è®­ç»ƒ | é¢„æµ‹æ…¢ã€é«˜ç»´å·® | å°æ•°æ®é›† |
| XGBoost | å‡†ç¡®ã€å¿«é€Ÿ | éœ€è°ƒå‚ã€é»‘ç›’ | ç«èµ›ã€è¡¨æ ¼æ•°æ® |

---

## å‚è€ƒèµ„æº

- ã€Šæœºå™¨å­¦ä¹ ã€‹- å‘¨å¿—å
- Scikit-learnå®˜æ–¹æ–‡æ¡£
- Kaggleç«èµ›

